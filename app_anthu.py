import streamlit as st
import pandas as pd
import numpy as np
import zipfile
import tempfile
import shutil
import os
import gc
import io
from pathlib import Path
from scipy.signal import find_peaks
from scipy.stats import median_abs_deviation

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# é¡µé¢é…ç½®
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="MALDI-TOF MS è·¨ä»ªå™¨ç»Ÿä¸€å¤„ç†å¹³å°",
    page_icon="ğŸ”¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CSS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("""
<style>
    .main-header {
        font-size: 2.2rem;
        font-weight: 700;
        color: #1f77b4;
        margin-bottom: 0.4rem;
        text-align: center;
    }
    .sub-header {
        font-size: 1.05rem;
        color: #666;
        margin-bottom: 1.6rem;
        text-align: center;
    }
    .phase-header {
        background: linear-gradient(90deg, #1f77b4 0%, #4a9eff 100%);
        color: white;
        padding: 0.7rem 1rem;
        border-radius: 8px;
        margin: 0.8rem 0;
        font-size: 1.15rem;
        font-weight: 600;
    }
    .info-box {
        background: #f0f7ff;
        border-left: 4px solid #1f77b4;
        padding: 0.8rem 1rem;
        border-radius: 0 6px 6px 0;
        margin: 0.6rem 0;
        font-size: 0.92rem;
    }
    .warn-box {
        background: #fff8e1;
        border-left: 4px solid #ff9800;
        padding: 0.8rem 1rem;
        border-radius: 0 6px 6px 0;
        margin: 0.6rem 0;
        font-size: 0.92rem;
    }
</style>
""", unsafe_allow_html=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# session state åˆå§‹åŒ–
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for key, default in {
    'template_mz': None,          # è®­ç»ƒé›†å½’å‡ºçš„åˆ†ç®±æ¨¡æ¿ (array)
    'bin_edges': None,            # åˆ†ç®±è¾¹ç•Œ
    'bin_size': 2,                # åˆ†ç®±ç²’åº¦
    'mz_min': 2000,
    'mz_max': 20500,
    'snr_threshold': 5,
    'prominence_factor': 1.0,
    'train_matrix': None,         # è®­ç»ƒé›†ç‰¹å¾çŸ©é˜µ DataFrame
    'template_ready': False,
}.items():
    if key not in st.session_state:
        st.session_state[key] = default


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ ¸å¿ƒè¯»å– / å¤„ç†å‡½æ•°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def detect_file_type(raw_bytes: bytes) -> str:
    """
    åˆ¤æ–­ txt æ˜¯å¸ƒé²å…‹(åŸå§‹å…‰è°±)è¿˜æ˜¯å®‰å›¾(å³°è¡¨).
    é€»è¾‘: å°è¯• gbk è§£ç ï¼Œçœ‹ç¬¬äºŒè¡Œæ˜¯å¦ä»¥ m/z å¼€å¤´ â†’ å®‰å›¾ï¼›å¦åˆ™ç¬¬ä¸€è¡Œç›´æ¥æ˜¯æ•°å­— â†’ å¸ƒé²å…‹
    """
    try:
        text = raw_bytes.decode('gbk')
    except:
        text = raw_bytes.decode('utf-8', errors='replace')
    lines = [l.strip() for l in text.splitlines() if l.strip()]
    if len(lines) < 2:
        return 'unknown'
    # å®‰å›¾ç¬¬äºŒè¡Œè¡¨å¤´å« m/z
    if 'm/z' in lines[1].lower() or 'm/z' in lines[0].lower():
        return 'anthu'
    # å¸ƒé²å…‹ç¬¬ä¸€è¡Œç›´æ¥æ˜¯ "æ•°å­— æ•°å­—"
    parts = lines[0].split()
    if len(parts) == 2:
        try:
            float(parts[0])
            float(parts[1])
            return 'bruker'
        except:
            pass
    return 'unknown'


def read_bruker(raw_bytes: bytes) -> pd.DataFrame:
    """è¯»å¸ƒé²å…‹åŸå§‹å…‰è°± â†’ DataFrame(mz, intensity)"""
    text = raw_bytes.decode('utf-8', errors='replace')
    rows = []
    for line in text.splitlines():
        parts = line.strip().split()
        if len(parts) == 2:
            try:
                rows.append((float(parts[0]), float(parts[1])))
            except ValueError:
                continue
    return pd.DataFrame(rows, columns=['mz', 'intensity'])


def read_anthu(raw_bytes: bytes) -> pd.DataFrame:
    """è¯»å®‰å›¾å³°è¡¨ â†’ DataFrame(mz, peak_height, peak_area, SNR, resolution)"""
    text = raw_bytes.decode('gbk', errors='replace')
    lines = text.splitlines()
    # è·³è¿‡ç¬¬1è¡Œ(è·¯å¾„)å’Œç¬¬2è¡Œ(è¡¨å¤´)
    data_lines = lines[2:]
    rows = []
    for line in data_lines:
        parts = [p.strip() for p in line.split('\t') if p.strip()]
        if len(parts) >= 5:
            try:
                rows.append([float(parts[0]), float(parts[1]),
                             float(parts[2]), float(parts[3]), float(parts[4])])
            except ValueError:
                continue
    df = pd.DataFrame(rows, columns=['mz', 'peak_height', 'peak_area', 'SNR', 'resolution'])
    return df


def bruker_to_peaks(df: pd.DataFrame, prominence_factor: float = 1.0) -> pd.DataFrame:
    """
    å¸ƒé²å…‹åŸå§‹å…‰è°± â†’ å³°æ£€æµ‹ â†’ (mz, intensity_sqrt)
    intensity åš sqrt å˜æ¢ï¼Œç”¨ MAD ä¼°å™ªï¼Œprominence = MAD * prominence_factor
    """
    mz = df['mz'].values
    intensity = df['intensity'].values.astype(float)
    intensity_sqrt = np.sqrt(intensity)

    noise = median_abs_deviation(intensity_sqrt)
    if noise < 1e-10:
        noise = 1.0

    peaks_idx, _ = find_peaks(
        intensity_sqrt,
        height=noise * 1.5,
        distance=3,
        prominence=noise * prominence_factor
    )
    return pd.DataFrame({
        'mz': mz[peaks_idx],
        'intensity': intensity_sqrt[peaks_idx]
    })


def anthu_to_peaks(df: pd.DataFrame, snr_threshold: int = 5) -> pd.DataFrame:
    """
    å®‰å›¾å³°è¡¨ â†’ ç­›é€‰(SNR >= threshold & area > 0) â†’ (mz, intensity_sqrt)
    peak_height åŒæ ·åš sqrtï¼Œä¿æŒè·¨ä»ªå™¨ä¸€è‡´
    """
    mask = (df['SNR'] >= snr_threshold) & (df['peak_area'] > 0)
    filt = df[mask].copy()
    return pd.DataFrame({
        'mz': filt['mz'].values,
        'intensity': np.sqrt(filt['peak_height'].values.astype(float))
    })


def peaks_to_bin_vector(peaks_df: pd.DataFrame,
                        bin_edges: np.ndarray,
                        n_bins: int) -> np.ndarray:
    """
    å°†å³°è¡¨æ˜ å°„åˆ°å›ºå®šåˆ†ç®±ç½‘æ ¼ â†’ é•¿åº¦ n_bins çš„å‘é‡ï¼ˆåŒ bin å–æœ€å¤§å€¼ï¼‰
    """
    vec = np.zeros(n_bins)
    mz_arr = peaks_df['mz'].values
    int_arr = peaks_df['intensity'].values
    # vectorized bin assignment
    indices = np.searchsorted(bin_edges, mz_arr, side='right') - 1
    valid = (indices >= 0) & (indices < n_bins)
    for idx, val in zip(indices[valid], int_arr[valid]):
        if val > vec[idx]:
            vec[idx] = val
    return vec


def tic_normalize(vec: np.ndarray) -> np.ndarray:
    """TICå½’ä¸€åŒ–: æ¯ä¸ªå€¼ / æ€»å’Œ"""
    s = vec.sum()
    return vec / s if s > 0 else vec


def process_single_file(raw_bytes: bytes,
                        bin_edges: np.ndarray,
                        n_bins: int,
                        snr_threshold: int,
                        prominence_factor: float) -> tuple:
    """
    ç»Ÿä¸€å…¥å£: è‡ªåŠ¨åˆ¤æ–­ç±»å‹ â†’ å³°æ£€æµ‹/è¯»å– â†’ åˆ†ç®± â†’ TICå½’ä¸€åŒ–
    è¿”å› (file_type, bin_vector_normalized, n_peaks_before_bin)
    """
    ftype = detect_file_type(raw_bytes)
    if ftype == 'bruker':
        raw_df = read_bruker(raw_bytes)
        peaks = bruker_to_peaks(raw_df, prominence_factor)
    elif ftype == 'anthu':
        raw_df = read_anthu(raw_bytes)
        peaks = anthu_to_peaks(raw_df, snr_threshold)
    else:
        return 'unknown', None, 0

    n_peaks = len(peaks)
    vec = peaks_to_bin_vector(peaks, bin_edges, n_bins)
    vec_norm = tic_normalize(vec)
    return ftype, vec_norm, n_peaks


def extract_zip(uploaded_zip) -> dict:
    """è§£æ ZIP â†’ {filename: raw_bytes}ï¼Œåªæå– .txt"""
    result = {}
    with zipfile.ZipFile(uploaded_zip, 'r') as z:
        for name in z.namelist():
            bn = Path(name).name
            if bn.lower().endswith('.txt') and not name.startswith('__MACOSX'):
                result[bn] = z.read(name)
    return result


def build_bin_edges(mz_min: int, mz_max: int, bin_size: int):
    edges = np.arange(mz_min, mz_max + bin_size, bin_size)
    centers = (edges[:-1] + edges[1:]) / 2
    n_bins = len(centers)
    return edges, centers, n_bins


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸»ç•Œé¢
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown('<div class="main-header">ğŸ”¬ MALDI-TOF MS è·¨ä»ªå™¨ç»Ÿä¸€å¤„ç†å¹³å°</div>', unsafe_allow_html=True)
st.markdown('<div class="sub-header">å¸ƒé²å…‹ & å®‰å›¾ æ··åˆæ•°æ® â†’ ç»Ÿä¸€åˆ†ç®±ç‰¹å¾çŸ©é˜µ</div>', unsafe_allow_html=True)

# â”€â”€ ä¾§è¾¹æ ï¼šå‚æ•° â”€â”€
with st.sidebar:
    st.header("âš™ï¸ å¤„ç†å‚æ•°")

    st.markdown('<div class="info-box">è¿™äº›å‚æ•°å¯¹<b>å¸ƒé²å…‹å’Œå®‰å›¾</b>åŒæ—¶ç”Ÿæ•ˆï¼Œå»ºè®®å…ˆç”¨é»˜è®¤å€¼è¯•ä¸€æ¬¡ã€‚</div>', unsafe_allow_html=True)

    bin_size = st.selectbox(
        "åˆ†ç®±ç²’åº¦ (Da)", [1, 2, 5], index=1,
        help="2 Da æ˜¯ MALDI-TOF é¢†åŸŸä¸»æµé€‰æ‹©ï¼Œä¸ MALDIquant binPeaks tolerance=2 ä¸€è‡´"
    )
    mz_min = st.number_input("m/z ä¸‹é™", value=2000, step=100)
    mz_max = st.number_input("m/z ä¸Šé™", value=20500, step=100)

    st.divider()
    st.subheader("ğŸ” å³°ç­›é€‰å‚æ•°")
    snr_threshold = st.slider(
        "å®‰å›¾ SNR é˜ˆå€¼", min_value=3, max_value=15, value=5,
        help="å®‰å›¾å³°è¡¨ä¸­ SNR < æ­¤å€¼çš„å³°ä¼šè¢«ä¸¢å¼ƒ"
    )
    prominence_factor = st.slider(
        "å¸ƒé²å…‹ prominence å€æ•°", min_value=0.5, max_value=3.0, value=1.0, step=0.1,
        help="å¸ƒé²å…‹å³°æ£€æµ‹çµæ•åº¦è°ƒèŠ‚ã€‚å€¼è¶Šå°æ£€æµ‹åˆ°çš„å³°è¶Šå¤š"
    )

    # åŒæ­¥åˆ° session
    st.session_state.bin_size = bin_size
    st.session_state.mz_min = mz_min
    st.session_state.mz_max = mz_max
    st.session_state.snr_threshold = snr_threshold
    st.session_state.prominence_factor = prominence_factor

    st.divider()
    st.header("ğŸ’¾ å†…å­˜ç®¡ç†")
    if st.button("ğŸ§¹ æ¸…ç†ç¼“å­˜", use_container_width=True):
        for k in list(st.session_state.keys()):
            if k not in ('bin_size','mz_min','mz_max','snr_threshold','prominence_factor'):
                st.session_state[k] = None
        st.session_state.template_ready = False
        gc.collect()
        st.success("å·²æ¸…ç†")
        st.rerun()

    st.divider()
    st.header("ğŸ“– æµç¨‹è¯´æ˜")
    st.markdown("""
    1. **é˜¶æ®µ1** â€” ä¸Šä¼ è®­ç»ƒé›† ZIPï¼ˆå¯æ··åˆå¸ƒé²å…‹/å®‰å›¾ txtï¼‰
       - è‡ªåŠ¨è¯†åˆ«æ¯ä¸ªæ–‡ä»¶çš„ä»ªå™¨ç±»å‹
       - ç»Ÿä¸€åˆ†ç®± â†’ ç”Ÿæˆç‰¹å¾æ¨¡æ¿
    2. **é˜¶æ®µ2** â€” ä¸Šä¼ éªŒè¯é›† ZIP
       - ä½¿ç”¨è®­ç»ƒé›†çš„ç‰¹å¾æ¨¡æ¿
       - è¾“å‡ºä¸è®­ç»ƒé›†**ç»´åº¦å®Œå…¨ä¸€è‡´**çš„ç‰¹å¾çŸ©é˜µ
    3. æœ€ç»ˆä¸¤ä¸ªçŸ©é˜µå¯ç›´æ¥åˆå¹¶è¿›æœºå™¨å­¦ä¹ æ¨¡å‹
    """)


# â”€â”€ ä¸»å†…å®¹ï¼šä¸¤ä¸ª Tab â”€â”€
tab1, tab2 = st.tabs(["ğŸ¯ é˜¶æ®µ1: è®­ç»ƒé›† â†’ å»ºç«‹æ¨¡æ¿", "ğŸ”„ é˜¶æ®µ2: éªŒè¯é›† â†’ åº”ç”¨æ¨¡æ¿"])

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é˜¶æ®µ1
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with tab1:
    st.markdown('<div class="phase-header">ğŸ“Š é˜¶æ®µ1: å¤„ç†è®­ç»ƒé›†ï¼Œå»ºç«‹åˆ†ç®±ç‰¹å¾æ¨¡æ¿</div>', unsafe_allow_html=True)
    st.markdown("""<div class="info-box">
    ä¸Šä¼ çš„ ZIP ä¸­å¯ä»¥<b>åŒæ—¶åŒ…å«å¸ƒé²å…‹å’Œå®‰å›¾çš„ txt</b>ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è¯†åˆ«å¹¶ç»Ÿä¸€å¤„ç†ã€‚<br>
    å¦‚æœä½ æœ‰æ ‡ç­¾ä¿¡æ¯ï¼ˆå“ªä¸ªæ–‡ä»¶å¯¹åº”å“ªä¸ªæ ·æœ¬/ç»„åˆ«ï¼‰ï¼Œå¯ä»¥åœ¨ä¸‹é¢é™„å¸¦ä¸€ä¸ª Excelã€‚
    </div>""", unsafe_allow_html=True)

    train_zip_upload = st.file_uploader("ä¸Šä¼ è®­ç»ƒé›† ZIP", type=['zip'], key='train_zip')

    # å¯é€‰ Excelï¼ˆæ ·æœ¬æ ‡ç­¾ï¼‰
    label_excel_upload = st.file_uploader(
        "ï¼ˆå¯é€‰ï¼‰ä¸Šä¼ æ ‡ç­¾ Excelï¼ˆåˆ—: file, groupï¼‰",
        type=['xlsx', 'xls'], key='label_excel'
    )

    if train_zip_upload:
        files_dict = extract_zip(train_zip_upload)
        if not files_dict:
            st.error("ZIP ä¸­æ²¡æœ‰æ‰¾åˆ° .txt æ–‡ä»¶")
        else:
            st.success(f"âœ… æ‰¾åˆ° {len(files_dict)} ä¸ª txt æ–‡ä»¶")

            # é¢„æ£€æŸ¥ï¼šè¯†åˆ«ç±»å‹
            type_counts = {'bruker': 0, 'anthu': 0, 'unknown': 0}
            for fn, raw in files_dict.items():
                t = detect_file_type(raw)
                type_counts[t] += 1
            col1, col2, col3 = st.columns(3)
            col1.metric("å¸ƒé²å…‹", type_counts['bruker'])
            col2.metric("å®‰å›¾", type_counts['anthu'])
            col3.metric("æœªè¯†åˆ«", type_counts['unknown'])

            if st.button("ğŸ¯ å¼€å§‹å¤„ç†è®­ç»ƒé›†", type="primary", use_container_width=True):
                progress = st.progress(0)
                status = st.empty()

                # è¯»å– label
                label_df = None
                if label_excel_upload:
                    try:
                        label_df = pd.read_excel(label_excel_upload)
                        # æ ‡å‡†åŒ–åˆ—å
                        label_df.columns = [c.strip().lower() for c in label_df.columns]
                        if 'file' in label_df.columns and 'group' in label_df.columns:
                            label_df['file'] = label_df['file'].astype(str).str.strip()
                        else:
                            st.warning("âš ï¸ Excel åˆ—åéœ€åŒ…å« 'file' å’Œ 'group'ï¼Œå°†å¿½ç•¥æ ‡ç­¾ä¿¡æ¯")
                            label_df = None
                    except Exception as e:
                        st.warning(f"âš ï¸ è¯»å– Excel å¤±è´¥: {e}ï¼Œå°†å¿½ç•¥æ ‡ç­¾ä¿¡æ¯")

                # æ„å»ºåˆ†ç®±ç½‘æ ¼
                edges, centers, n_bins = build_bin_edges(
                    st.session_state.mz_min,
                    st.session_state.mz_max,
                    st.session_state.bin_size
                )
                st.session_state.bin_edges = edges

                status.text("ğŸ”„ æ­£åœ¨é€æ–‡ä»¶å¤„ç†...")
                progress.progress(10)

                records = []      # (filename, type, n_peaks, vec)
                total = len(files_dict)

                for i, (fn, raw) in enumerate(files_dict.items()):
                    ftype, vec_norm, n_peaks = process_single_file(
                        raw, edges, n_bins,
                        st.session_state.snr_threshold,
                        st.session_state.prominence_factor
                    )
                    if vec_norm is not None:
                        records.append((fn, ftype, n_peaks, vec_norm))
                    progress.progress(10 + int(70 * (i + 1) / total))

                status.text("ğŸ“¦ æ„å»ºç‰¹å¾çŸ©é˜µ...")

                # ç¡®å®šéé›¶åˆ—ï¼ˆè®­ç»ƒé›†ä¸­è‡³å°‘ä¸€ä¸ªæ ·æœ¬æœ‰å³°çš„ binï¼‰
                mat = np.vstack([r[3] for r in records])
                nonzero_mask = mat.sum(axis=0) > 0
                nonzero_indices = np.where(nonzero_mask)[0]
                active_centers = centers[nonzero_indices]

                # è£å‰ªåˆ°éé›¶åˆ—
                mat_trimmed = mat[:, nonzero_indices]

                # åˆ—å
                col_names = [f"mz_{c:.1f}" for c in active_centers]

                # æ„å»º DataFrame
                train_df = pd.DataFrame(mat_trimmed, columns=col_names)
                train_df.insert(0, 'sample', [r[0] for r in records])
                train_df.insert(1, 'instrument', [r[1] for r in records])
                train_df.insert(2, 'n_peaks_detected', [r[2] for r in records])

                # åŠ  group æ ‡ç­¾
                if label_df is not None:
                    label_map = dict(zip(label_df['file'], label_df['group']))
                    train_df.insert(3, 'group',
                                    train_df['sample'].map(label_map).fillna('unknown'))

                # ä¿å­˜åˆ° session
                st.session_state.train_matrix = train_df
                st.session_state.template_mz = active_centers  # æ¨¡æ¿ m/z
                st.session_state.template_ready = True

                progress.progress(100)
                status.text("âœ… å®Œæˆï¼")

                import time; time.sleep(0.4)
                progress.empty()
                status.empty()

                # â”€â”€ ç»“æœå±•ç¤º â”€â”€
                st.success("âœ… è®­ç»ƒé›†å¤„ç†å®Œæˆï¼Œç‰¹å¾æ¨¡æ¿å·²å»ºç«‹ï¼")

                c1, c2, c3, c4 = st.columns(4)
                c1.metric("æ ·æœ¬æ•°", len(train_df))
                c2.metric("æ´»è·ƒç‰¹å¾æ•°", len(active_centers))
                c3.metric("m/z èŒƒå›´", f"{active_centers.min():.0f}~{active_centers.max():.0f}")
                c4.metric("åˆ†ç®±ç²’åº¦", f"{st.session_state.bin_size} Da")

                # ä»ªå™¨åˆ†å¸ƒ
                st.subheader("ğŸ“‹ æ ·æœ¬æ˜ç»†")
                st.dataframe(
                    train_df[['sample','instrument','n_peaks_detected'] +
                             (['group'] if 'group' in train_df.columns else [])],
                    use_container_width=True, hide_index=True
                )

                # ç‰¹å¾çŸ©é˜µé¢„è§ˆ
                with st.expander("ğŸ“Š ç‰¹å¾çŸ©é˜µé¢„è§ˆï¼ˆå‰10åˆ—ï¼‰"):
                    preview_cols = ['sample','instrument'] + col_names[:10]
                    st.dataframe(train_df[preview_cols].round(6), use_container_width=True, hide_index=True)

                # ä¸‹è½½
                st.divider()
                st.subheader("ğŸ“¥ ä¸‹è½½")
                c1, c2 = st.columns(2)

                # ä¸‹è½½ç‰¹å¾çŸ©é˜µï¼ˆå»æ‰ instrument å’Œ n_peaks è¾…åŠ©åˆ—ï¼‰
                export_cols = ['sample'] + (['group'] if 'group' in train_df.columns else []) + col_names
                c1.download_button(
                    "ğŸ“Š è®­ç»ƒé›†ç‰¹å¾çŸ©é˜µ CSV",
                    data=train_df[export_cols].to_csv(index=False),
                    file_name="train_feature_matrix.csv",
                    mime="text/csv",
                    use_container_width=True
                )

                # ä¸‹è½½æ¨¡æ¿ï¼ˆm/zåˆ—è¡¨ï¼‰
                template_export = pd.DataFrame({
                    'bin_center': active_centers,
                    'feature_name': col_names
                })
                c2.download_button(
                    "ğŸ¯ ç‰¹å¾æ¨¡æ¿ CSV",
                    data=template_export.to_csv(index=False),
                    file_name="feature_template.csv",
                    mime="text/csv",
                    use_container_width=True
                )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é˜¶æ®µ2
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with tab2:
    st.markdown('<div class="phase-header">ğŸ”„ é˜¶æ®µ2: å¤„ç†éªŒè¯é›†ï¼ˆä½¿ç”¨è®­ç»ƒé›†æ¨¡æ¿ï¼‰</div>', unsafe_allow_html=True)

    if not st.session_state.template_ready:
        st.markdown('<div class="warn-box">âš ï¸ è¯·å…ˆå®Œæˆé˜¶æ®µ1ï¼Œå»ºç«‹ç‰¹å¾æ¨¡æ¿åæ‰èƒ½å¤„ç†éªŒè¯é›†ã€‚</div>', unsafe_allow_html=True)
    else:
        active_centers = st.session_state.template_mz
        col_names = [f"mz_{c:.1f}" for c in active_centers]

        st.success(f"âœ… ç‰¹å¾æ¨¡æ¿å°±ç»ªï¼š{len(active_centers)} ä¸ªç‰¹å¾")
        st.markdown("""<div class="info-box">
        éªŒè¯é›†åŒæ ·å¯ä»¥æ··åˆå¸ƒé²å…‹å’Œå®‰å›¾ txtã€‚è¾“å‡ºçš„ç‰¹å¾ç»´åº¦ä¼šä¸è®­ç»ƒé›†<b>å®Œå…¨ä¸€è‡´</b>ã€‚
        </div>""", unsafe_allow_html=True)

        valid_zip_upload = st.file_uploader("ä¸Šä¼ éªŒè¯é›† ZIP", type=['zip'], key='valid_zip')

        if valid_zip_upload:
            files_dict = extract_zip(valid_zip_upload)
            if not files_dict:
                st.error("ZIP ä¸­æ²¡æœ‰æ‰¾åˆ° .txt æ–‡ä»¶")
            else:
                st.success(f"âœ… æ‰¾åˆ° {len(files_dict)} ä¸ª txt æ–‡ä»¶")

                type_counts = {'bruker': 0, 'anthu': 0, 'unknown': 0}
                for fn, raw in files_dict.items():
                    type_counts[detect_file_type(raw)] += 1
                c1, c2, c3 = st.columns(3)
                c1.metric("å¸ƒé²å…‹", type_counts['bruker'])
                c2.metric("å®‰å›¾", type_counts['anthu'])
                c3.metric("æœªè¯†åˆ«", type_counts['unknown'])

                # å¯é€‰æ ‡ç­¾
                valid_label_upload = st.file_uploader(
                    "ï¼ˆå¯é€‰ï¼‰éªŒè¯é›†æ ‡ç­¾ Excelï¼ˆåˆ—: file, groupï¼‰",
                    type=['xlsx','xls'], key='valid_label'
                )

                if st.button("ğŸ”„ å¼€å§‹å¤„ç†éªŒè¯é›†", type="primary", use_container_width=True):
                    progress = st.progress(0)
                    status = st.empty()

                    # é‡å»ºå’Œè®­ç»ƒé›†ä¸€è‡´çš„ bin_edgesï¼ˆç”¨å…¨èŒƒå›´ï¼Œåé¢æ˜ å°„åˆ°æ¨¡æ¿åˆ—ï¼‰
                    edges, centers, n_bins = build_bin_edges(
                        st.session_state.mz_min,
                        st.session_state.mz_max,
                        st.session_state.bin_size
                    )

                    # æ¨¡æ¿ m/z â†’ åœ¨ centers ä¸­å¯¹åº”çš„ index
                    template_indices = []
                    for tc in active_centers:
                        idx = np.argmin(np.abs(centers - tc))
                        template_indices.append(idx)
                    template_indices = np.array(template_indices)

                    status.text("ğŸ”„ é€æ–‡ä»¶å¤„ç†...")
                    progress.progress(10)

                    records = []
                    total = len(files_dict)

                    for i, (fn, raw) in enumerate(files_dict.items()):
                        ftype, vec_norm, n_peaks = process_single_file(
                            raw, edges, n_bins,
                            st.session_state.snr_threshold,
                            st.session_state.prominence_factor
                        )
                        if vec_norm is not None:
                            # åªå–æ¨¡æ¿å¯¹åº”çš„åˆ—
                            vec_template = vec_norm[template_indices]
                            records.append((fn, ftype, n_peaks, vec_template))
                        progress.progress(10 + int(70 * (i + 1) / total))

                    status.text("ğŸ“¦ æ„å»ºç‰¹å¾çŸ©é˜µ...")

                    mat = np.vstack([r[3] for r in records])
                    valid_df = pd.DataFrame(mat, columns=col_names)
                    valid_df.insert(0, 'sample', [r[0] for r in records])
                    valid_df.insert(1, 'instrument', [r[1] for r in records])
                    valid_df.insert(2, 'n_peaks_detected', [r[2] for r in records])

                    # åŠ æ ‡ç­¾
                    if valid_label_upload:
                        try:
                            ldf = pd.read_excel(valid_label_upload)
                            ldf.columns = [c.strip().lower() for c in ldf.columns]
                            if 'file' in ldf.columns and 'group' in ldf.columns:
                                ldf['file'] = ldf['file'].astype(str).str.strip()
                                lmap = dict(zip(ldf['file'], ldf['group']))
                                valid_df.insert(3, 'group',
                                                valid_df['sample'].map(lmap).fillna('unknown'))
                        except:
                            pass

                    progress.progress(100)
                    status.text("âœ… å®Œæˆï¼")

                    import time; time.sleep(0.4)
                    progress.empty()
                    status.empty()

                    # â”€â”€ ç»“æœ â”€â”€
                    st.success("âœ… éªŒè¯é›†å¤„ç†å®Œæˆï¼ç‰¹å¾ç»´åº¦ä¸è®­ç»ƒé›†ä¸€è‡´ï¼")

                    c1, c2, c3 = st.columns(3)
                    c1.metric("æ ·æœ¬æ•°", len(valid_df))
                    c2.metric("ç‰¹å¾æ•°", len(col_names))
                    c3.metric("ç‰¹å¾ä¸€è‡´æ€§", "âœ… ä¸è®­ç»ƒé›†ä¸€è‡´")

                    st.subheader("ğŸ“‹ æ ·æœ¬æ˜ç»†")
                    st.dataframe(
                        valid_df[['sample','instrument','n_peaks_detected'] +
                                 (['group'] if 'group' in valid_df.columns else [])],
                        use_container_width=True, hide_index=True
                    )

                    with st.expander("ğŸ“Š ç‰¹å¾çŸ©é˜µé¢„è§ˆï¼ˆå‰10åˆ—ï¼‰"):
                        preview_cols = ['sample','instrument'] + col_names[:10]
                        st.dataframe(valid_df[preview_cols].round(6), use_container_width=True, hide_index=True)

                    # ä¸‹è½½
                    st.divider()
                    export_cols = ['sample'] + (['group'] if 'group' in valid_df.columns else []) + col_names
                    st.download_button(
                        "ğŸ“Š ä¸‹è½½éªŒè¯é›†ç‰¹å¾çŸ©é˜µ CSV",
                        data=valid_df[export_cols].to_csv(index=False),
                        file_name="valid_feature_matrix.csv",
                        mime="text/csv",
                        use_container_width=True
                    )

                    del valid_df
                    gc.collect()

# â”€â”€ åº•éƒ¨ â”€â”€
st.divider()
st.markdown("<div style='text-align:center;color:#aaa;font-size:0.85rem;'>MALDI-TOF MS è·¨ä»ªå™¨ç»Ÿä¸€å¤„ç†å¹³å° Â· å¸ƒé²å…‹ & å®‰å›¾</div>", unsafe_allow_html=True)
