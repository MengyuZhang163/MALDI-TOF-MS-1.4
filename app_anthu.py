import streamlit as st
import pandas as pd
import numpy as np
import zipfile
import gc
import time
from pathlib import Path

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# é¡µé¢é…ç½®
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="MALDI-TOF MS è·¨ä»ªå™¨ç»Ÿä¸€å¤„ç†å¹³å°",
    page_icon="ğŸ”¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CSS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("""
<style>
    .main-header {
        font-size: 2.2rem;
        font-weight: 700;
        color: #1f77b4;
        margin-bottom: 0.4rem;
        text-align: center;
    }
    .sub-header {
        font-size: 1.05rem;
        color: #666;
        margin-bottom: 1.6rem;
        text-align: center;
    }
    .phase-header {
        background: linear-gradient(90deg, #1f77b4 0%, #4a9eff 100%);
        color: white;
        padding: 0.7rem 1rem;
        border-radius: 8px;
        margin: 0.8rem 0;
        font-size: 1.15rem;
        font-weight: 600;
    }
    .info-box {
        background: #f0f7ff;
        border-left: 4px solid #1f77b4;
        padding: 0.8rem 1rem;
        border-radius: 0 6px 6px 0;
        margin: 0.6rem 0;
        font-size: 0.92rem;
    }
    .warn-box {
        background: #fff8e1;
        border-left: 4px solid #ff9800;
        padding: 0.8rem 1rem;
        border-radius: 0 6px 6px 0;
        margin: 0.6rem 0;
        font-size: 0.92rem;
    }
</style>
""", unsafe_allow_html=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# session state åˆå§‹åŒ–
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for key, default in {
    'template_cols': None,
    'template_mz_values': None,
    'template_ready': False,
    'snr_threshold': 5,
    'align_tolerance': 5,
    'strain_threshold_pct': 90,
}.items():
    if key not in st.session_state:
        st.session_state[key] = default


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ ¸å¿ƒå‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_zip_txt(uploaded_zip) -> dict:
    """è§£æ ZIP â†’ {filename: raw_bytes}ï¼Œåªæå– .txt"""
    result = {}
    with zipfile.ZipFile(uploaded_zip, 'r') as z:
        for name in z.namelist():
            bn = Path(name).name
            if bn.lower().endswith('.txt') and not name.startswith('__MACOSX') and bn:
                result[bn] = z.read(name)
    return result


def read_anthu_txt(raw_bytes: bytes) -> pd.DataFrame:
    """
    è¯»å®‰å›¾å•ä¸ª txt â†’ DataFrame(mz, peak_height, peak_area, SNR, resolution)
    ç¼–ç  gbkï¼Œç¬¬1è¡Œè·¯å¾„ï¼Œç¬¬2è¡Œè¡¨å¤´ï¼Œç¬¬3è¡Œèµ·æ•°æ®
    """
    text = raw_bytes.decode('gbk', errors='replace')
    lines = text.splitlines()
    rows = []
    for line in lines[2:]:
        parts = [p.strip() for p in line.split('\t') if p.strip()]
        if len(parts) >= 5:
            try:
                rows.append([float(parts[0]), float(parts[1]),
                             float(parts[2]), float(parts[3]), float(parts[4])])
            except ValueError:
                continue
    return pd.DataFrame(rows, columns=['mz', 'peak_height', 'peak_area', 'SNR', 'resolution'])


def filter_anthu_peaks(df: pd.DataFrame, snr_threshold: int = 5) -> pd.DataFrame:
    """å®‰å›¾å³°è¡¨ç­›é€‰: SNR >= threshold ä¸” peak_area > 0"""
    mask = (df['SNR'] >= snr_threshold) & (df['peak_area'] > 0)
    return df[mask].reset_index(drop=True)


def extract_strain_id(filename: str) -> str:
    """ä»å®‰å›¾æ–‡ä»¶åæå–èŒæ ªIDã€‚  D1_F250905209902_spectrum-1.txt â†’ D1"""
    return filename.split('_')[0]


def anthu_to_feature_vector(peaks_df: pd.DataFrame,
                            template_mz: np.ndarray,
                            tolerance: float) -> np.ndarray:
    """
    å•ä¸ªå®‰å›¾æ ·æœ¬å³°è¡¨ â†’ æŒ‰æ¨¡æ¿å¯¹é½çš„ç‰¹å¾å‘é‡ï¼ˆTICå½’ä¸€åŒ–ï¼‰
    æ¯ä¸ªæ¨¡æ¿ mz æ‰¾è·ç¦» <= tolerance çš„å³°ä¸­ peak_height æœ€å¤§å€¼ï¼›æœªå‘½ä¸­å¡« 0
    """
    n = len(template_mz)
    vec = np.zeros(n)
    if len(peaks_df) == 0:
        return vec

    atu_mz = peaks_df['mz'].values
    atu_height = peaks_df['peak_height'].values.astype(float)

    for i, tmz in enumerate(template_mz):
        diffs = np.abs(atu_mz - tmz)
        mask = diffs <= tolerance
        if mask.any():
            vec[i] = atu_height[mask].max()

    s = vec.sum()
    if s > 0:
        vec = vec / s
    return vec


def cluster_mz(mz_array: np.ndarray, tolerance: float) -> np.ndarray:
    """è·ç¦» <= tolerance çš„ mz èšä¸ºä¸€ç°‡ï¼Œå–ä¸­ä½æ•°ã€‚è¾“å…¥éœ€å·²æ’åºæˆ–ä¼šå†…éƒ¨æ’åºã€‚"""
    if len(mz_array) == 0:
        return np.array([])
    sorted_mz = np.sort(mz_array)
    clusters, cur = [], [sorted_mz[0]]
    for mz in sorted_mz[1:]:
        if mz - cur[-1] <= tolerance:
            cur.append(mz)
        else:
            clusters.append(cur)
            cur = [mz]
    clusters.append(cur)
    return np.array([np.median(c) for c in clusters])


def find_anthu_unique_candidates(bruker_csv_df: pd.DataFrame,
                                 all_anthu_peaks: list,
                                 tolerance: float) -> np.ndarray:
    """
    æ‰¾å‡ºæ‰€æœ‰å®‰å›¾å³°ä¸­å¯¹é½ä¸åˆ°å¸ƒé²å…‹ä»»ä½•ç‰¹å¾çš„å€™é€‰ mzï¼Œèšç±»å»é‡åè¿”å›ä¸­ä½æ•°æ•°ç»„ã€‚
    """
    brk_cols = [c for c in bruker_csv_df.columns if c.startswith('mz_')]
    brk_mz = np.array([float(c.replace('mz_', '')) for c in brk_cols])

    all_atu_mz = []
    for fn, peaks_df in all_anthu_peaks:
        all_atu_mz.extend(peaks_df['mz'].values.tolist())
    if len(all_atu_mz) == 0:
        return np.array([])

    all_atu_mz = np.array(all_atu_mz)
    # ç­›é€‰ï¼šå¯¹é½ä¸åˆ°å¸ƒé²å…‹ä»»ä½•ç‰¹å¾çš„å³°
    candidates = []
    for am in all_atu_mz:
        if len(brk_mz) == 0 or np.min(np.abs(brk_mz - am)) > tolerance:
            candidates.append(am)
    if len(candidates) == 0:
        return np.array([])
    return cluster_mz(np.array(candidates), tolerance)


def compute_strain_detection(all_anthu_peaks: list,
                             candidate_mz: np.ndarray,
                             tolerance: float) -> pd.DataFrame:
    """
    å¯¹æ¯ä¸ªå€™é€‰ç‰¹å¾ mzï¼Œç»Ÿè®¡åœ¨å¤šå°‘èŒæ ªä¸­è‡³å°‘æœ‰1ä¸ªæ ·æœ¬æ£€æµ‹åˆ°ã€‚
    è¿”å› DataFrame: mz, col_name, n_strains_detected, total_strains, detection_pct,
                    n_samples_detected, total_samples
    """
    if len(candidate_mz) == 0:
        return pd.DataFrame(columns=['mz', 'col_name', 'n_strains_detected',
                                     'total_strains', 'detection_pct',
                                     'n_samples_detected', 'total_samples'])

    # æŒ‰èŒæ ªåˆ†ç»„
    strain_samples: dict[str, list] = {}
    for fn, peaks_df in all_anthu_peaks:
        sid = extract_strain_id(fn)
        strain_samples.setdefault(sid, []).append(peaks_df)

    total_strains = len(strain_samples)
    total_samples = len(all_anthu_peaks)

    records = []
    for cmz in candidate_mz:
        strains_hit = 0
        samples_hit = 0
        for sid, sample_list in strain_samples.items():
            strain_has = False
            for pdf in sample_list:
                if len(pdf) > 0 and np.any(np.abs(pdf['mz'].values - cmz) <= tolerance):
                    strain_has = True
                    samples_hit += 1
            if strain_has:
                strains_hit += 1

        records.append({
            'mz': round(float(cmz), 1),
            'col_name': f"mz_{int(round(cmz))}",
            'n_strains_detected': strains_hit,
            'total_strains': total_strains,
            'detection_pct': round(strains_hit / total_strains * 100, 1) if total_strains > 0 else 0.0,
            'n_samples_detected': samples_hit,
            'total_samples': total_samples,
        })
    return pd.DataFrame(records)


def build_unified_template(bruker_csv_df: pd.DataFrame,
                           candidate_mz_kept: np.ndarray):
    """å¸ƒé²å…‹ç‰¹å¾ + ç»ç­›é€‰ä¿ç•™çš„å®‰å›¾ç‰¹å¾ â†’ ç»Ÿä¸€æ¨¡æ¿ï¼ˆå‡åºï¼‰"""
    brk_cols = [c for c in bruker_csv_df.columns if c.startswith('mz_')]
    brk_mz = np.array([float(c.replace('mz_', '')) for c in brk_cols])

    if len(candidate_mz_kept) == 0:
        return brk_cols, brk_mz

    all_mz = np.concatenate([brk_mz, candidate_mz_kept])
    sort_idx = np.argsort(all_mz)
    unified_mz = all_mz[sort_idx]
    unified_cols = [f"mz_{int(round(m))}" for m in unified_mz]
    return unified_cols, unified_mz


def bruker_csv_to_unified(bruker_csv_df: pd.DataFrame,
                          unified_mz: np.ndarray,
                          tolerance: float) -> np.ndarray:
    """å¸ƒé²å…‹ CSV æ¯è¡Œæ˜ å°„åˆ°ç»Ÿä¸€æ¨¡æ¿ï¼Œæ–°å¢å®‰å›¾åˆ—å¡« 0ï¼Œæ•°å€¼ä¿æŒåŸæ ·ã€‚"""
    brk_cols = [c for c in bruker_csv_df.columns if c.startswith('mz_')]
    brk_mz = np.array([float(c.replace('mz_', '')) for c in brk_cols])
    brk_values = bruker_csv_df[brk_cols].values.astype(float)

    out = np.zeros((len(bruker_csv_df), len(unified_mz)))
    for j, bmz in enumerate(brk_mz):
        diffs = np.abs(unified_mz - bmz)
        nearest = np.argmin(diffs)
        if diffs[nearest] <= tolerance:
            out[:, nearest] = brk_values[:, j]
    return out


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¾§è¾¹æ 
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with st.sidebar:
    st.header("âš™ï¸ å¤„ç†å‚æ•°")
    st.markdown('<div class="info-box">å‚æ•°è°ƒæ•´æ”¾åœ¨<b>é˜¶æ®µ1ä¸»åŒºåŸŸ</b>ä¸‹æ–¹ï¼Œæ–¹ä¾¿éšæ—¶ä¿®æ”¹ã€‚</div>', unsafe_allow_html=True)

    st.divider()
    st.header("ğŸ’¾ å†…å­˜ç®¡ç†")
    if st.button("ğŸ§¹ æ¸…ç†ç¼“å­˜ï¼ˆä¿ç•™æ¨¡æ¿ï¼‰", use_container_width=True):
        keys_to_keep = {'template_cols', 'template_mz_values', 'template_ready',
                        'snr_threshold', 'align_tolerance', 'strain_threshold_pct'}
        for k in list(st.session_state.keys()):
            if k not in keys_to_keep:
                del st.session_state[k]
        gc.collect()
        st.success("å·²æ¸…ç†")
        st.rerun()
    if st.button("ğŸ—‘ï¸ å®Œå…¨æ¸…ç©º", use_container_width=True):
        st.session_state.clear()
        gc.collect()
        st.success("å·²æ¸…ç©º")
        st.rerun()

    st.divider()
    st.header("ğŸ“– æµç¨‹è¯´æ˜")
    st.markdown("""
    **é˜¶æ®µ1ï¼ˆè®­ç»ƒé›†ï¼‰ï¼š**
    - ä¸Šä¼ å¸ƒé²å…‹ CSV + å®‰å›¾ ZIP
    - å¸ƒé²å…‹ 91 åˆ—ä½œä¸ºæ¨¡æ¿æ ¸å¿ƒï¼Œä¿æŒä¸å˜
    - å®‰å›¾ç‹¬æœ‰å³°æŒ‰**èŒæ ªæ£€æµ‹ç‡**ç­›é€‰ï¼š
      åªæœ‰åœ¨è¶³å¤Ÿå¤šèŒæ ªä¸­éƒ½èƒ½æ£€æµ‹åˆ°çš„å³°æ‰è¿½åŠ ä¸ºæ–°ç‰¹å¾ï¼Œç æ‰ç¨€ç–å™ªå£°åˆ—
    - è¾“å‡ºï¼šç­›é€‰åçš„ç»Ÿä¸€ç‰¹å¾çŸ©é˜µ + æ¨¡æ¿

    **é˜¶æ®µ2ï¼ˆéªŒè¯é›†ï¼‰ï¼š**
    - åªéœ€ä¸Šä¼ å®‰å›¾ ZIP
    - ç”¨é˜¶æ®µ1çš„æ¨¡æ¿å¯¹é½ï¼Œç»´åº¦ä¸€è‡´
    - æ¨¡æ¿ä¹‹å¤–çš„å³°è‡ªåŠ¨å¿½ç•¥

    **èŒæ ªæ£€æµ‹ç‡é€»è¾‘ï¼š**
    æ¯ä¸ªå®‰å›¾ç‹¬æœ‰ç‰¹å¾ï¼Œæ£€æŸ¥åœ¨å¤šå°‘èŒæ ªä¸­è‡³å°‘æœ‰ 1 ä¸ªé‡å¤æ ·æœ¬æ£€æµ‹åˆ°ã€‚
    ä½äºé˜ˆå€¼çš„ç‰¹å¾ç æ‰ï¼Œé¿å…æ¨¡å‹å™ªå£°ã€‚
    """)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸»ç•Œé¢
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
st.markdown('<div class="main-header">ğŸ”¬ MALDI-TOF MS è·¨ä»ªå™¨ç»Ÿä¸€å¤„ç†å¹³å°</div>', unsafe_allow_html=True)
st.markdown('<div class="sub-header">å¸ƒé²å…‹ CSV + å®‰å›¾ TXT â†’ èŒæ ªäº¤é›†ç­›é€‰ â†’ ç»Ÿä¸€ç‰¹å¾çŸ©é˜µ</div>', unsafe_allow_html=True)

tab1, tab2 = st.tabs(["ğŸ¯ é˜¶æ®µ1: è®­ç»ƒé›† â†’ å»ºç«‹æ¨¡æ¿", "ğŸ”„ é˜¶æ®µ2: éªŒè¯é›† â†’ åº”ç”¨æ¨¡æ¿"])


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é˜¶æ®µ1
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with tab1:
    st.markdown('<div class="phase-header">ğŸ“Š é˜¶æ®µ1: è®­ç»ƒé›†å¤„ç†ï¼Œå»ºç«‹ç»Ÿä¸€ç‰¹å¾æ¨¡æ¿</div>', unsafe_allow_html=True)
    st.markdown("""<div class="info-box">
    <b>å¸ƒé²å…‹</b>ï¼šä¸Šä¼ ä¹‹å‰ç”¨ R å¤„ç†å¥½çš„ CSVï¼ˆéœ€å« <code>group</code> åˆ—å’Œ <code>mz_xxxx</code> ç‰¹å¾åˆ—ï¼‰<br>
    <b>å®‰å›¾</b>ï¼šä¸Šä¼ åŒ…å«æ‰€æœ‰ txt çš„ ZIP å‹ç¼©åŒ…<br>
    ç³»ç»Ÿä»¥å¸ƒé²å…‹ç‰¹å¾ä¸ºæ ¸å¿ƒæ¨¡æ¿ï¼Œå®‰å›¾ç‹¬æœ‰å³°ç»<b>èŒæ ªæ£€æµ‹ç‡ç­›é€‰</b>åæ‰è¿½åŠ ä¸ºæ–°ç‰¹å¾ï¼Œå¤§å¹…å‡å°‘å™ªå£°åˆ—ã€‚
    </div>""", unsafe_allow_html=True)

    # â”€â”€ ä¸Šä¼ åŒº â”€â”€
    col_brk, col_atu = st.columns(2)
    with col_brk:
        brk_csv_upload = st.file_uploader("ğŸ“ å¸ƒé²å…‹ CSV", type=['csv'], key='brk_csv',
                                          help="R å¤„ç†åçš„å³°å¼ºåº¦çŸ©é˜µ")
    with col_atu:
        atu_zip_upload = st.file_uploader("ğŸ“ å®‰å›¾ ZIP", type=['zip'], key='atu_zip',
                                          help="åŒ…å«æ‰€æœ‰å®‰å›¾ txt çš„å‹ç¼©åŒ…")

    # â”€â”€ é¢„æ£€æŸ¥ â”€â”€
    brk_df = None
    atu_files = {}

    if brk_csv_upload:
        brk_df = pd.read_csv(brk_csv_upload)
        brk_mz_cols = [c for c in brk_df.columns if c.startswith('mz_')]
        has_group = 'group' in brk_df.columns
        st.success("âœ… å¸ƒé²å…‹ CSV è¯»å…¥æˆåŠŸ")
        c1, c2, c3 = st.columns(3)
        c1.metric("æ ·æœ¬æ•°", len(brk_df))
        c2.metric("ç‰¹å¾æ•°", len(brk_mz_cols))
        c3.metric("å« group åˆ—", "âœ… æ˜¯" if has_group else "âŒ å¦")

    if atu_zip_upload:
        atu_files = extract_zip_txt(atu_zip_upload)
        st.success(f"âœ… å®‰å›¾ ZIP è¯»å…¥æˆåŠŸï¼š{len(atu_files)} ä¸ª txt")
        strains_detected = set(extract_strain_id(fn) for fn in atu_files.keys())
        st.info(f"ğŸ§¬ æ£€æµ‹åˆ° {len(strains_detected)} ä¸ªèŒæ ªï¼Œæ¯æ ªçº¦ {len(atu_files) // max(len(strains_detected), 1)} ä¸ªé‡å¤")

    # â”€â”€ å‚æ•°æ§ä»¶ â”€â”€
    st.divider()
    p1, p2, p3 = st.columns(3)
    with p1:
        phase1_snr = st.slider(
            "ğŸ” å®‰å›¾ SNR é˜ˆå€¼", min_value=3, max_value=15,
            value=st.session_state.snr_threshold,
            help="å®‰å›¾å³°è¡¨ä¸­ SNR < æ­¤å€¼çš„å³°ä¼šè¢«ä¸¢å¼ƒ"
        )
        st.session_state.snr_threshold = phase1_snr
    with p2:
        phase1_tol = st.slider(
            "ğŸ“ å¯¹é½å®¹å·® (Da)", min_value=1, max_value=15,
            value=st.session_state.align_tolerance,
            help="å®‰å›¾å³°ä¸å¸ƒé²å…‹ç‰¹å¾ mz çš„æœ€å¤§å…è®¸åå·®"
        )
        st.session_state.align_tolerance = phase1_tol
    with p3:
        phase1_strain_pct = st.slider(
            "ğŸ§¬ èŒæ ªæ£€æµ‹ç‡é˜ˆå€¼ (%)", min_value=50, max_value=100,
            value=st.session_state.strain_threshold_pct, step=5,
            help="å®‰å›¾ç‹¬æœ‰ç‰¹å¾å¿…é¡»åœ¨æ­¤æ¯”ä¾‹ä»¥ä¸Šçš„èŒæ ªä¸­æ£€æµ‹åˆ°æ‰èƒ½ä¿ç•™ã€‚è¶Šé«˜è¶Šä¸¥æ ¼ï¼Œå™ªå£°è¶Šå°‘ï¼›è¶Šä½ä¿ç•™ç‰¹å¾è¶Šå¤šã€‚"
        )
        st.session_state.strain_threshold_pct = phase1_strain_pct

    # â”€â”€ å¤„ç†æŒ‰é’® â”€â”€
    can_run = (brk_df is not None) and (len(atu_files) > 0)
    if not can_run:
        st.markdown('<div class="warn-box">âš ï¸ è¯·åŒæ—¶ä¸Šä¼ å¸ƒé²å…‹ CSV å’Œå®‰å›¾ ZIP æ‰èƒ½å¼€å§‹å¤„ç†ã€‚</div>', unsafe_allow_html=True)

    if can_run and st.button("ğŸ¯ å¼€å§‹å¤„ç†è®­ç»ƒé›†", type="primary", use_container_width=True):

        progress = st.progress(0)
        status = st.empty()
        tolerance    = st.session_state.align_tolerance
        snr_thresh   = st.session_state.snr_threshold
        strain_pct   = st.session_state.strain_threshold_pct

        # â”€â”€ Step 1: è¯»å–å¹¶ç­›é€‰æ‰€æœ‰å®‰å›¾å³° â”€â”€
        status.text("ğŸ“– Step 1/5: è¯»å–å®‰å›¾ txt å¹¶ç­›é€‰å³°...")
        progress.progress(5)

        all_anthu_peaks = []
        atu_read_info   = []
        for fn, raw in atu_files.items():
            raw_df  = read_anthu_txt(raw)
            filt_df = filter_anthu_peaks(raw_df, snr_thresh)
            all_anthu_peaks.append((fn, filt_df))
            atu_read_info.append((fn, len(raw_df), len(filt_df)))

        progress.progress(12)

        # â”€â”€ Step 2: æ‰¾å®‰å›¾ç‹¬æœ‰å€™é€‰ç‰¹å¾ â”€â”€
        status.text("ğŸ” Step 2/5: è¯†åˆ«å®‰å›¾ç‹¬æœ‰å€™é€‰ç‰¹å¾ï¼ˆèšç±»å»é‡ï¼‰...")
        candidate_mz_all = find_anthu_unique_candidates(brk_df, all_anthu_peaks, tolerance)
        n_candidates_raw = len(candidate_mz_all)
        progress.progress(25)

        # â”€â”€ Step 3: èŒæ ªæ£€æµ‹ç‡è®¡ç®— + ç­›é€‰ â”€â”€
        status.text("ğŸ§¬ Step 3/5: æŒ‰èŒæ ªæ£€æµ‹ç‡ç­›é€‰å€™é€‰ç‰¹å¾...")
        detection_df = compute_strain_detection(all_anthu_peaks, candidate_mz_all, tolerance)

        if len(detection_df) > 0:
            mask_keep       = detection_df['detection_pct'] >= strain_pct
            kept_df         = detection_df[mask_keep].copy()
            dropped_df      = detection_df[~mask_keep].copy()
            candidate_mz_kept = kept_df['mz'].values.astype(float)
        else:
            kept_df           = detection_df.copy()
            dropped_df        = detection_df.copy()
            candidate_mz_kept = np.array([])

        progress.progress(45)

        # â”€â”€ Step 4: æ„å»ºç»Ÿä¸€æ¨¡æ¿ â”€â”€
        status.text("ğŸ“ Step 4/5: æ„å»ºç»Ÿä¸€ç‰¹å¾æ¨¡æ¿...")
        unified_cols, unified_mz = build_unified_template(brk_df, candidate_mz_kept)
        n_brk_features  = len([c for c in brk_df.columns if c.startswith('mz_')])
        n_new_features  = len(unified_cols) - n_brk_features
        progress.progress(55)

        # â”€â”€ Step 5: æ˜ å°„æ‰€æœ‰æ ·æœ¬ â”€â”€
        status.text("ğŸ“Š Step 5/5: æ˜ å°„æ ·æœ¬åˆ°ç»Ÿä¸€ç‰¹å¾çŸ©é˜µ...")

        # å¸ƒé²å…‹æ˜ å°„
        brk_matrix = bruker_csv_to_unified(brk_df, unified_mz, tolerance)
        brk_out = pd.DataFrame(brk_matrix, columns=unified_cols)

        # â”€â”€ TIC å½’ä¸€åŒ–ï¼šæ¯è¡Œ / è¡Œæ€»å’Œ â†’ å’Œå®‰å›¾ä¸€è‡´ï¼ˆæ¯”ä¾‹å«ä¹‰ï¼‰ â”€â”€
        brk_rowsum = brk_out[unified_cols].sum(axis=1)
        brk_out[unified_cols] = brk_out[unified_cols].div(brk_rowsum.replace(0, np.nan), axis=0).fillna(0)

        brk_out.insert(0, 'sample',
                       brk_df['group'].astype(str).values if 'group' in brk_df.columns
                       else [f"bruker_{i}" for i in range(len(brk_df))])
        brk_out.insert(1, 'instrument', 'bruker')
        progress.progress(70)

        # å®‰å›¾æ˜ å°„
        atu_rows = []
        for fn, filt_df in all_anthu_peaks:
            atu_rows.append(anthu_to_feature_vector(filt_df, unified_mz, tolerance))

        atu_out = pd.DataFrame(np.vstack(atu_rows), columns=unified_cols)
        atu_out.insert(0, 'sample', [r[0] for r in all_anthu_peaks])
        atu_out.insert(1, 'instrument', 'anthu')
        progress.progress(85)

        # â”€â”€ åˆå¹¶ & ä¿å­˜æ¨¡æ¿ â”€â”€
        combined = pd.concat([brk_out, atu_out], ignore_index=True)
        st.session_state.template_cols      = unified_cols
        st.session_state.template_mz_values = unified_mz
        st.session_state.template_ready     = True

        progress.progress(100)
        status.text("âœ… å®Œæˆï¼")
        time.sleep(0.4)
        progress.empty()
        status.empty()

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ç»“æœå±•ç¤º
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        st.success("âœ… è®­ç»ƒé›†å¤„ç†å®Œæˆï¼Œç»Ÿä¸€ç‰¹å¾æ¨¡æ¿å·²å»ºç«‹ï¼")

        # â”€â”€ æ ¸å¿ƒæŒ‡æ ‡ â”€â”€
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("æ€»æ ·æœ¬æ•°",   len(combined))
        c2.metric("æœ€ç»ˆç‰¹å¾æ•°", len(unified_cols),
                  delta=f"{n_new_features:+d} (å®‰å›¾æ–°å¢)")
        c3.metric("å¸ƒé²å…‹æ ¸å¿ƒç‰¹å¾", n_brk_features)
        c4.metric("å®‰å›¾ç­›é€‰åæ–°å¢", n_new_features)

        c1, c2, c3 = st.columns(3)
        c1.metric("å¸ƒé²å…‹æ ·æœ¬æ•°", len(brk_out))
        c2.metric("å®‰å›¾æ ·æœ¬æ•°",  len(atu_out))
        c3.metric("å®‰å›¾èŒæ ªæ•°",  len(set(extract_strain_id(fn) for fn, _ in all_anthu_peaks)))

        # â”€â”€ TIC å½’ä¸€åŒ–éªŒè¯ â”€â”€
        st.divider()
        st.markdown("### ğŸ“Š TIC å½’ä¸€åŒ–éªŒè¯ï¼ˆä¸¤ç§ä»ªå™¨å¼ºåº¦å·²å¯¹é½ï¼‰")
        brk_final_rowsum = brk_out[unified_cols].sum(axis=1)
        atu_final_rowsum = atu_out[unified_cols].sum(axis=1)
        nc1, nc2 = st.columns(2)
        nc1.metric("å¸ƒé²å…‹ è¡Œæ€»å’Œ", f"{brk_final_rowsum.mean():.4f}",
                   help="å½’ä¸€åŒ–åæ¯è¡Œå’Œ=1ï¼Œè¡¨ç¤ºå„å³°å æœ¬æ ·æœ¬æ€»ä¿¡å·çš„æ¯”ä¾‹")
        nc2.metric("å®‰å›¾   è¡Œæ€»å’Œ", f"{atu_final_rowsum.mean():.4f}",
                   help="å’Œå¸ƒé²å…‹åŒä¸€åŸºå‡†ï¼Œç›´æ¥å¯æ¯”")

        # â”€â”€ èŒæ ªç­›é€‰æŠ¥å‘Š â”€â”€
        st.divider()
        st.markdown("### ğŸ§¬ èŒæ ªæ£€æµ‹ç‡ç­›é€‰æŠ¥å‘Š")

        total_strains  = int(detection_df['total_strains'].iloc[0]) if len(detection_df) > 0 else 0
        strain_thresh_n = int(np.ceil(strain_pct / 100.0 * total_strains))

        rc1, rc2, rc3 = st.columns(3)
        rc1.metric("å€™é€‰ç‰¹å¾æ€»æ•°ï¼ˆå»é‡åï¼‰", n_candidates_raw)
        rc2.metric(f"ä¿ç•™ï¼ˆæ£€æµ‹ç‡ â‰¥ {strain_pct}%ï¼Œå³ â‰¥ {strain_thresh_n} èŒæ ªï¼‰",
                   f"{len(kept_df)} ä¸ª",
                   delta=f"{len(kept_df) - n_candidates_raw}")
        rc3.metric("ç æ‰ï¼ˆå¤ªç¨€ç–ï¼‰", f"{len(dropped_df)} ä¸ª")

        # ä¿ç•™ç‰¹å¾æ˜ç»†
        if len(kept_df) > 0:
            with st.expander(f"âœ… ä¿ç•™çš„ {len(kept_df)} ä¸ªå®‰å›¾ç‰¹å¾", expanded=True):
                show = kept_df[['col_name','mz','n_strains_detected','total_strains',
                                'detection_pct','n_samples_detected','total_samples']].copy()
                show.columns = ['ç‰¹å¾å','mzå€¼','æ£€æµ‹èŒæ ªæ•°','æ€»èŒæ ªæ•°','æ£€æµ‹ç‡(%)','æ£€æµ‹æ ·æœ¬æ•°','æ€»æ ·æœ¬æ•°']
                st.dataframe(show.sort_values('æ£€æµ‹ç‡(%)', ascending=False).reset_index(drop=True),
                             use_container_width=True, hide_index=True)

        # ç æ‰ç‰¹å¾æ˜ç»†
        if len(dropped_df) > 0:
            with st.expander(f"âŒ ç æ‰çš„ {len(dropped_df)} ä¸ªç‰¹å¾ï¼ˆæ£€æµ‹ç‡å¤ªä½ï¼‰"):
                show = dropped_df[['col_name','mz','n_strains_detected','total_strains',
                                   'detection_pct','n_samples_detected','total_samples']].copy()
                show.columns = ['ç‰¹å¾å','mzå€¼','æ£€æµ‹èŒæ ªæ•°','æ€»èŒæ ªæ•°','æ£€æµ‹ç‡(%)','æ£€æµ‹æ ·æœ¬æ•°','æ€»æ ·æœ¬æ•°']
                st.dataframe(show.sort_values('æ£€æµ‹ç‡(%)', ascending=False).reset_index(drop=True),
                             use_container_width=True, hide_index=True)

        # â”€â”€ å®‰å›¾æ–‡ä»¶æ˜ç»† â”€â”€
        st.divider()
        st.subheader("ğŸ“‹ å®‰å›¾æ–‡ä»¶æ˜ç»†")
        st.dataframe(
            pd.DataFrame(atu_read_info, columns=['æ–‡ä»¶å','åŸå§‹å³°æ•°', f'ç­›é€‰åå³°æ•°(SNRâ‰¥{snr_thresh})']),
            use_container_width=True, hide_index=True
        )

        # â”€â”€ ç‰¹å¾çŸ©é˜µé¢„è§ˆ â”€â”€
        with st.expander("ğŸ“Š åˆå¹¶ç‰¹å¾çŸ©é˜µé¢„è§ˆï¼ˆå‰10åˆ—ï¼‰"):
            st.dataframe(combined[['sample','instrument'] + unified_cols[:10]].round(6),
                         use_container_width=True, hide_index=True)

        # â”€â”€ ä¸‹è½½ â”€â”€
        st.divider()
        st.subheader("ğŸ“¥ ä¸‹è½½")
        dl1, dl2 = st.columns(2)
        export_cols = ['sample'] + unified_cols

        dl1.download_button(
            "ğŸ“Š è®­ç»ƒé›†ç»Ÿä¸€ç‰¹å¾çŸ©é˜µ CSV",
            data=combined[export_cols].to_csv(index=False),
            file_name="train_feature_matrix_unified.csv",
            mime="text/csv", use_container_width=True
        )
        dl2.download_button(
            "ğŸ¯ ç‰¹å¾æ¨¡æ¿ CSV",
            data=pd.DataFrame({'feature_name': unified_cols,
                               'mz_value': unified_mz.round(1)}).to_csv(index=False),
            file_name="feature_template.csv",
            mime="text/csv", use_container_width=True
        )

        del combined, brk_out, atu_out, brk_matrix
        gc.collect()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é˜¶æ®µ2
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with tab2:
    st.markdown('<div class="phase-header">ğŸ”„ é˜¶æ®µ2: éªŒè¯é›†å¤„ç†ï¼ˆä½¿ç”¨è®­ç»ƒé›†æ¨¡æ¿ï¼‰</div>', unsafe_allow_html=True)

    if not st.session_state.template_ready:
        st.markdown('<div class="warn-box">âš ï¸ è¯·å…ˆå®Œæˆé˜¶æ®µ1ï¼Œå»ºç«‹ç‰¹å¾æ¨¡æ¿åæ‰èƒ½å¤„ç†éªŒè¯é›†ã€‚</div>', unsafe_allow_html=True)
    else:
        unified_cols = st.session_state.template_cols
        unified_mz   = st.session_state.template_mz_values

        st.success(f"âœ… ç‰¹å¾æ¨¡æ¿å°±ç»ªï¼š{len(unified_cols)} ä¸ªç‰¹å¾")
        st.markdown("""<div class="info-box">
        ä¸Šä¼ å®‰å›¾ txt çš„ ZIPï¼Œç³»ç»ŸæŒ‰è®­ç»ƒé›†æ¨¡æ¿å¯¹é½ï¼Œè¾“å‡ºä¸è®­ç»ƒé›†<b>ç»´åº¦å®Œå…¨ä¸€è‡´</b>çš„ç‰¹å¾çŸ©é˜µã€‚<br>
        æ¨¡æ¿ä¹‹å¤–çš„å³°è‡ªåŠ¨å¿½ç•¥ï¼›å¸ƒé²å…‹ç‹¬æœ‰çš„é«˜ mz ç‰¹å¾åœ¨å®‰å›¾æ ·æœ¬ä¸­å¡« 0ã€‚
        </div>""", unsafe_allow_html=True)

        valid_zip_upload = st.file_uploader("ğŸ“ å®‰å›¾éªŒè¯é›† ZIP", type=['zip'], key='valid_zip')

        if valid_zip_upload:
            valid_files = extract_zip_txt(valid_zip_upload)
            if not valid_files:
                st.error("ZIP ä¸­æ²¡æœ‰æ‰¾åˆ° .txt æ–‡ä»¶")
            else:
                st.success(f"âœ… æ‰¾åˆ° {len(valid_files)} ä¸ª txt æ–‡ä»¶")

                # å‚æ•°æ§ä»¶
                p1, p2 = st.columns(2)
                with p1:
                    phase2_snr = st.slider(
                        "ğŸ” å®‰å›¾ SNR é˜ˆå€¼", min_value=3, max_value=15,
                        value=st.session_state.snr_threshold, key='valid_snr',
                        help="å®‰å›¾å³°è¡¨ä¸­ SNR < æ­¤å€¼çš„å³°ä¼šè¢«ä¸¢å¼ƒ"
                    )
                    st.session_state.snr_threshold = phase2_snr
                with p2:
                    phase2_tol = st.slider(
                        "ğŸ“ å¯¹é½å®¹å·® (Da)", min_value=1, max_value=15,
                        value=st.session_state.align_tolerance, key='valid_tol',
                        help="å®‰å›¾å³°ä¸æ¨¡æ¿ç‰¹å¾ mz çš„æœ€å¤§å…è®¸åå·®ã€‚è¶…å‡ºéƒ¨åˆ†è‡ªåŠ¨å¿½ç•¥"
                    )
                    st.session_state.align_tolerance = phase2_tol

                if st.button("ğŸ”„ å¼€å§‹å¤„ç†éªŒè¯é›†", type="primary", use_container_width=True):
                    progress = st.progress(0)
                    status   = st.empty()
                    tolerance  = st.session_state.align_tolerance
                    snr_thresh = st.session_state.snr_threshold

                    status.text("ğŸ“– è¯»å–å¹¶å¤„ç†å®‰å›¾ txt...")
                    progress.progress(10)

                    valid_rows, valid_info = [], []
                    total = len(valid_files)
                    for i, (fn, raw) in enumerate(valid_files.items()):
                        raw_df  = read_anthu_txt(raw)
                        filt_df = filter_anthu_peaks(raw_df, snr_thresh)
                        valid_rows.append(anthu_to_feature_vector(filt_df, unified_mz, tolerance))
                        valid_info.append((fn, len(raw_df), len(filt_df)))
                        progress.progress(10 + int(75 * (i + 1) / total))

                    status.text("ğŸ“¦ æ„å»ºè¾“å‡ºçŸ©é˜µ...")
                    valid_df = pd.DataFrame(np.vstack(valid_rows), columns=unified_cols)
                    valid_df.insert(0, 'sample',     [r[0] for r in valid_info])
                    valid_df.insert(1, 'instrument', 'anthu')

                    progress.progress(100)
                    status.text("âœ… å®Œæˆï¼")
                    time.sleep(0.4)
                    progress.empty()
                    status.empty()

                    # â•â•â• ç»“æœå±•ç¤º â•â•â•
                    st.success("âœ… éªŒè¯é›†å¤„ç†å®Œæˆï¼ç‰¹å¾ç»´åº¦ä¸è®­ç»ƒé›†ä¸€è‡´ï¼")

                    c1, c2, c3 = st.columns(3)
                    c1.metric("æ ·æœ¬æ•°",   len(valid_df))
                    c2.metric("ç‰¹å¾æ•°",   len(unified_cols))
                    c3.metric("ç‰¹å¾ä¸€è‡´æ€§", "âœ… ä¸è®­ç»ƒé›†ä¸€è‡´")

                    st.subheader("ğŸ“‹ æ–‡ä»¶æ˜ç»†")
                    st.dataframe(
                        pd.DataFrame(valid_info, columns=['æ–‡ä»¶å','åŸå§‹å³°æ•°', f'ç­›é€‰åå³°æ•°(SNRâ‰¥{snr_thresh})']),
                        use_container_width=True, hide_index=True
                    )

                    # éé›¶ç‰¹å¾ç»Ÿè®¡
                    feat_data      = valid_df[unified_cols].astype(float)
                    nonzero_counts = (feat_data > 0).sum(axis=1)
                    with st.expander("ğŸ“Š å„æ ·æœ¬éé›¶ç‰¹å¾æ•°ä¸è¦†ç›–ç‡"):
                        st.dataframe(pd.DataFrame({
                            'æ–‡ä»¶å':   valid_df['sample'].values,
                            'éé›¶ç‰¹å¾æ•°': nonzero_counts.values,
                            'è¦†ç›–ç‡':   (nonzero_counts / len(unified_cols) * 100).round(1).astype(str) + '%'
                        }), use_container_width=True, hide_index=True)

                    with st.expander("ğŸ“Š ç‰¹å¾çŸ©é˜µé¢„è§ˆï¼ˆå‰10åˆ—ï¼‰"):
                        st.dataframe(valid_df[['sample'] + unified_cols[:10]].round(6),
                                     use_container_width=True, hide_index=True)

                    # â”€â”€ ä¸‹è½½ â”€â”€
                    st.divider()
                    st.download_button(
                        "ğŸ“Š ä¸‹è½½éªŒè¯é›†ç‰¹å¾çŸ©é˜µ CSV",
                        data=valid_df[['sample'] + unified_cols].to_csv(index=False),
                        file_name="valid_feature_matrix_unified.csv",
                        mime="text/csv", use_container_width=True
                    )

                    del valid_df, feat_data
                    gc.collect()


# â”€â”€ åº•éƒ¨ â”€â”€
st.divider()
st.markdown("<div style='text-align:center;color:#aaa;font-size:0.85rem;'>MALDI-TOF MS è·¨ä»ªå™¨ç»Ÿä¸€å¤„ç†å¹³å° Â· å¸ƒé²å…‹æ ¸å¿ƒ + å®‰å›¾èŒæ ªäº¤é›†ç­›é€‰</div>", unsafe_allow_html=True)
